{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## first make sure you have initiate the bert-server by open a terminal and type in the below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert-server-start -model_dir ./multi_cased_L-12_H-768_A-12/ -num_worker=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()\n",
    "out=bc.encode(['hur kan jag får support'])\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "server config:\n",
      "                        client\t=\t1934b3e7-ca0f-42ab-94ef-f8dd01a2017b\n",
      "                   num_process\t=\t2                             \n",
      "          ventilator -> worker\t=\t['tcp://127.0.0.1:59235', 'tcp://127.0.0.1:59236', 'tcp://127.0.0.1:59237', 'tcp://127.0.0.1:59238', 'tcp://127.0.0.1:59239', 'tcp://127.0.0.1:59240', 'tcp://127.0.0.1:59241', 'tcp://127.0.0.1:59242']\n",
      "                worker -> sink\t=\ttcp://127.0.0.1:59257         \n",
      "           ventilator <-> sink\t=\ttcp://127.0.0.1:59234         \n",
      "           server_current_time\t=\t2019-06-11 19:49:31.173275    \n",
      "                     statistic\t=\t{'num_data_request': 4, 'num_total_seq': 6, 'num_sys_request': 6, 'num_total_request': 10, 'num_total_client': 6, 'num_active_client': 2, 'avg_request_per_client': 1.6666666666666667, 'min_request_per_client': 1, 'max_request_per_client': 2, 'num_min_request_per_client': 2, 'num_max_request_per_client': 4, 'avg_size_per_request': 2.0, 'min_size_per_request': 1, 'max_size_per_request': 3, 'num_min_size_per_request': 1, 'num_max_size_per_request': 1, 'avg_last_two_interval': 399.55010657500003, 'min_last_two_interval': 17.934812200000124, 'max_last_two_interval': 1038.2380342000001, 'num_min_last_two_interval': 1, 'num_max_last_two_interval': 1, 'avg_request_per_second': 0.016097781541683485, 'min_request_per_second': 0.0009631702625598147, 'max_request_per_second': 0.05575748376110641, 'num_min_request_per_second': 1, 'num_max_request_per_second': 1}\n",
      "                    device_map\t=\t[]                            \n",
      "         num_concurrent_socket\t=\t8                             \n",
      "                     ckpt_name\t=\tbert_model.ckpt               \n",
      "                   config_name\t=\tbert_config.json              \n",
      "                          cors\t=\t*                             \n",
      "                           cpu\t=\tFalse                         \n",
      "                 do_lower_case\t=\tTrue                          \n",
      "            fixed_embed_length\t=\tFalse                         \n",
      "                          fp16\t=\tFalse                         \n",
      "           gpu_memory_fraction\t=\t0.5                           \n",
      "                 graph_tmp_dir\t=\tNone                          \n",
      "              http_max_connect\t=\t10                            \n",
      "                     http_port\t=\tNone                          \n",
      "                  mask_cls_sep\t=\tFalse                         \n",
      "                max_batch_size\t=\t256                           \n",
      "                   max_seq_len\t=\t25                            \n",
      "                     model_dir\t=\t./multi_cased_L-12_H-768_A-12/\n",
      "                    num_worker\t=\t1                             \n",
      "                 pooling_layer\t=\t[-2]                          \n",
      "              pooling_strategy\t=\t2                             \n",
      "                          port\t=\t5555                          \n",
      "                      port_out\t=\t5556                          \n",
      "                 prefetch_size\t=\t10                            \n",
      "           priority_batch_size\t=\t16                            \n",
      "         show_tokens_to_client\t=\tFalse                         \n",
      "               tuned_model_dir\t=\tNone                          \n",
      "                       verbose\t=\tFalse                         \n",
      "                           xla\t=\tFalse                         \n",
      "            tensorflow_version\t=\t['1', '13', '1']              \n",
      "                python_version\t=\t3.6.8 |Anaconda, Inc.| (default, Feb 21 2019, 18:30:04) [MSC v.1916 64 bit (AMD64)]\n",
      "                server_version\t=\t1.9.1                         \n",
      "                 pyzmq_version\t=\t18.0.0                        \n",
      "                   zmq_version\t=\t4.3.1                         \n",
      "             server_start_time\t=\t2019-06-11 19:20:52.446937    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zecharpy\\AppData\\Local\\Continuum\\anaconda3\\envs\\bert_py36\\lib\\site-packages\\bert_serving\\client\\__init__.py:286: UserWarning: some of your sentences have more tokens than \"max_seq_len=25\" set on the server, as consequence you may get less-accurate or truncated embeddings.\n",
      "here is what you can do:\n",
      "- disable the length-check by create a new \"BertClient(check_length=False)\" when you do not want to display this warning\n",
      "- or, start a new server with a larger \"max_seq_len\"\n",
      "  '- or, start a new server with a larger \"max_seq_len\"' % self.length_limit)\n",
      "C:\\Users\\zecharpy\\AppData\\Local\\Continuum\\anaconda3\\envs\\bert_py36\\lib\\site-packages\\bert_serving\\client\\__init__.py:295: UserWarning: \"show_tokens=True\", but the server does not support showing tokenization info to clients.\n",
      "here is what you can do:\n",
      "- start a new server with \"bert-serving-start -show_tokens_to_client ...\"\n",
      "- or, use \"encode(show_tokens=False)\"\n",
      "  warnings.warn('\"show_tokens=True\", but the server does not support showing tokenization info to clients.\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding         18 sentences\t0.65s\t  27 samples/s\t   706 tokens/s\n",
      "encoding         36 sentences\t1.28s\t  28 samples/s\t   718 tokens/s\n",
      "encoding         72 sentences\t2.40s\t  30 samples/s\t   768 tokens/s\n",
      "encoding        144 sentences\t4.40s\t  32 samples/s\t   838 tokens/s\n",
      "encoding        288 sentences\t9.08s\t  31 samples/s\t   811 tokens/s\n",
      "encoding        576 sentences\t21.18s\t  27 samples/s\t   696 tokens/s\n",
      "encoding       1152 sentences\t51.07s\t  22 samples/s\t   577 tokens/s\n",
      "encoding       2304 sentences\t78.77s\t  29 samples/s\t   749 tokens/s\n",
      "encoding       4608 sentences\t157.76s\t  29 samples/s\t   748 tokens/s\n",
      "encoding       9216 sentences\t323.72s\t  28 samples/s\t   729 tokens/s\n"
     ]
    }
   ],
   "source": [
    "bc = BertClient( show_server_config=False)\n",
    "# encode a list of strings\n",
    "with open('README.md') as fp:\n",
    "    data = [v for v in fp if v.strip()][:512]\n",
    "    num_tokens = sum(len([vv for vv in v.split() if vv.strip()]) for v in data)\n",
    "\n",
    "show_tokens = True\n",
    "bc.encode(data)  # warm-up GPU\n",
    "for j in range(10):\n",
    "    tmp = data * (2 ** j)\n",
    "    c_num_tokens = num_tokens * (2 ** j)\n",
    "    start_t = time.time()\n",
    "    bc.encode(tmp, show_tokens=show_tokens)\n",
    "    time_t = time.time() - start_t\n",
    "    print('encoding %10d sentences\\t%.2fs\\t%4d samples/s\\t%6d tokens/s' %\n",
    "          (len(tmp), time_t, int(len(tmp) / time_t), int(c_num_tokens / time_t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test the dictionary for QnA maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('Hur aktiverar jag mina Office-produkter?', 'Klicka på länkarna nedan för mer information om aktivering av Office-produkter'), ('Vad händer när nedladdningen är slutförd?', 'När nedladdningen slutförts går du till platsen där du sparade filen och dubbelklickar på filikonen för att starta installationen.'), ('Hur får jag support?', 'ring 020 120 32 05'), ('Vad är en ”incident”?', 'En \"incident\" är ett enskilt supportproblem och de rimliga åtgärder som krävs för att lösa problemet.'), ('Kan man använda mobiler ihop med Teams?', 'Självklart! Du kan via Teamsklienten få det att ringa ut på mobiltelefonen.'), ('Kan man spela in samtal?', 'Ja, samtalsinspelning kan göras med hjälp av tredjepartsapplikation för externa samtal.'), ('Växelfunktioner?', 'Ja självklart.'), ('Kan man integrera sin mail i Teams?', 'Det finns idag en integration via ”butiken” i Teams för e-post men inget officiellt tillägg från Microsoft.'), ('Hur starta man en gruppchatt', 'Välj ny chatt i Teams längst upp i Teams.'), ('Hur Lägga jag till personer i konversationen', 'klickar du på Lägg till personer'), ('Hur kan jag ta bort en chatt i grupper ?', 'Det är bra om du rensar upp i röran. Även om du inte kan ta bort chattkonversationer just nu, så jobbar vi på det. Håll ögonen öppna.'), ('Hur skapa man emoji ?', 'Emoji är ett bra sätt att lägga till några bra i kommunikationen! Klicka på Emoji')])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from bert_serving.client import BertClient\n",
    "from termcolor import colored\n",
    "\n",
    "def whatisthis(s):\n",
    "        if isinstance(s, str):\n",
    "            #print(\"ordinary string\")\n",
    "            s=s.replace('Ã¥','å')\n",
    "            s=s.replace('Ã¶','ö')\n",
    "            s=s.replace('Ã¤','ä')\n",
    "            #print(s)\n",
    "            return s\n",
    "\n",
    "        elif isinstance(s, unicode):\n",
    "            print(\"unicode string\")\n",
    "            return s\n",
    "        else:\n",
    "            print(\"not a string\")\n",
    "            return s\n",
    "def qna_dictionary(prefix_q='Q:',prefix_a='A:',file='qna.txt'):\n",
    "    prefix_q =prefix_q\n",
    "    prefix_a =prefix_a\n",
    "    topk = 1\n",
    "    indexer=0\n",
    "    qna={}\n",
    "    with open(file, encoding='utf-8-sig') as fp:\n",
    "        lines = fp.readlines()\n",
    "        lines=[line for line in lines if len(line)>1]\n",
    "        cnt=0\n",
    "        for l in lines:\n",
    "\n",
    "            #print('{}'.format(str(cnt)),l)\n",
    "            if l.startswith(prefix_q):\n",
    "                #print(l)\n",
    "                question=l.replace(prefix_q, '').strip()\n",
    "                question=whatisthis(question)\n",
    "                #print(\"Q\",question)\n",
    "                answer=lines[cnt+1].replace(prefix_a, '').strip()\n",
    "                answer=whatisthis(answer)\n",
    "                #print(\"A\",answer)\n",
    "                qna[question]=answer\n",
    "                #print(cnt,l)\n",
    "            cnt+=1\n",
    "        return qna\n",
    "\n",
    "    \n",
    "t=qna_dictionary()\n",
    "\n",
    "t.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting qna_example.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile qna_example.py \n",
    "import numpy as np\n",
    "from bert_serving.client import BertClient\n",
    "from termcolor import colored\n",
    "\n",
    "prefix_q ='Q:' \n",
    "answer ='A:'\n",
    "topk = 1\n",
    "\n",
    "def whatisthis(s):\n",
    "        if isinstance(s, str):\n",
    "            #print(\"ordinary string\")\n",
    "            s=s.replace('Ã¥','å')\n",
    "            s=s.replace('Ã¶','ö')\n",
    "            s=s.replace('Ã¤','ä')\n",
    "            #print(s)\n",
    "            return s\n",
    "\n",
    "        elif isinstance(s, unicode):\n",
    "            print(\"unicode string\")\n",
    "            return s\n",
    "        else:\n",
    "            print(\"not a string\")\n",
    "            return s\n",
    "def qna_dictionary(prefix_q='Q:',prefix_a='A:',file='qna.txt'):\n",
    "    prefix_q =prefix_q\n",
    "    prefix_a =prefix_a\n",
    "    topk = 1\n",
    "    indexer=0\n",
    "    qna={}\n",
    "    with open(file, encoding='utf-8-sig') as fp:\n",
    "        lines = fp.readlines()\n",
    "        lines=[line for line in lines if len(line)>1]\n",
    "        cnt=0\n",
    "        for l in lines:\n",
    "\n",
    "            #print('{}'.format(str(cnt)),l)\n",
    "            if l.startswith(prefix_q):\n",
    "                #print(l)\n",
    "                question=l.replace(prefix_q, '').strip()\n",
    "                question=whatisthis(question)\n",
    "                print(\"Q\",question)\n",
    "                answer=lines[cnt+1].replace(prefix_a, '').strip()\n",
    "                answer=whatisthis(answer)\n",
    "                print(\"A\",answer)\n",
    "                qna[question]=answer\n",
    "                #print(cnt,l)\n",
    "            cnt+=1\n",
    "        return qna\n",
    "qna=qna_dictionary()\n",
    "with open('qna.txt', encoding='utf-8-sig') as fp:\n",
    "    questions = [v.replace(prefix_q, '').strip() for v in fp if v.strip() and v.startswith(prefix_q)]\n",
    "    print('%d questions loaded, avg. len of %d' % (len(questions), np.mean([len(d.split()) for d in questions])))\n",
    "\n",
    "with BertClient() as bc:\n",
    "    doc_vecs = bc.encode(questions)\n",
    "\n",
    "    while True:\n",
    "        query = input(colored('din fråga: ', 'green'))\n",
    "        query_vec = bc.encode([query])[0]\n",
    "        # compute normalized dot product as score\n",
    "        score = np.sum(query_vec * doc_vecs, axis=1) / np.linalg.norm(doc_vecs, axis=1)\n",
    "        topk_idx = np.argsort(score)[::-1][:topk]\n",
    "        #print('top %d questions similar to your query \\n\"%s\"' % (topk, colored(query, 'green')))\n",
    "        #print('top %d answer to your question %s')%(topk,colored(qna[whatisthis(query)],'yellow'))\n",
    "\n",
    "        for idx in topk_idx:\n",
    "            print('most similar question in our QnA list is \\n---> %s\\t%s' % (colored('%.1f' % score[idx], 'cyan'), colored(questions[idx], 'yellow')))\n",
    "            q=whatisthis(questions[idx])\n",
    "            \n",
    "            print('answer from QnA maker txt \\n ---> %s' % (qna[q]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hur får jag support\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
